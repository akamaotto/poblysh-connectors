# database Specification

## Purpose
Define the database layer for the Connectors service, including Postgres connectivity, migration execution, and the baseline tenants schema required for future capabilities.
## Requirements
### Requirement: Postgres Connection Management
The system SHALL use SeaORM to manage a connection pool to Postgres using `POBLYSH_DATABASE_URL`. Connections MUST be established during startup and made available to request handlers and background tasks.

Pool behavior (MVP):
- Default max connections: 10 (configurable)
- Acquire timeout: 5s (configurable)
- Retry on transient errors with backoff during startup initialization

#### Scenario: Connects with valid database URL
- GIVEN `POBLYSH_DATABASE_URL` points to a reachable Postgres instance
- WHEN the service starts
- THEN a `DatabaseConnection` pool is created and stored in application state

#### Scenario: Invalid credentials fail fast
- GIVEN `POBLYSH_DATABASE_URL` contains wrong credentials
- WHEN the service starts
- THEN initialization fails with an actionable error indicating authentication failed

### Requirement: Migration Runner
The system MUST include SeaORM Migrator and apply pending migrations. For profiles `local` and `test`, migrations SHALL run automatically on startup. For other profiles (e.g., `dev`, `prod`), migrations SHOULD be executed explicitly via a CLI entry or deployment step.

#### Scenario: Local profile runs migrations on startup
- GIVEN `POBLYSH_PROFILE=local`
- AND the database is empty
- WHEN the service starts
- THEN the migrator applies all pending migrations successfully

#### Scenario: Idempotent reruns
- GIVEN the database is already at the latest migration
- WHEN the migrator runs again
- THEN it completes without altering existing schema or data

### Requirement: Baseline Schema (Tenants)
The baseline schema SHALL include a `tenants` table to support tenant isolation across all future entities.

Columns (MVP):
- `id UUID PRIMARY KEY NOT NULL` (generated by application as UUID v4)
- `name TEXT NULL`
- `created_at TIMESTAMPTZ NOT NULL DEFAULT now()`

Constraints:
- Primary key on `id`

#### Scenario: Insert tenant succeeds
- GIVEN a newly generated UUID v4 `id`
- WHEN inserting into `tenants (id, name)`
- THEN the row is created and `created_at` is set automatically

#### Scenario: Duplicate id rejected
- GIVEN an existing row in `tenants` with `id = X`
- WHEN inserting another row with `id = X`
- THEN the operation fails due to primary key violation

### Requirement: Provider Entity Schema
The system SHALL define a `providers` table representing supported integration providers (global catalog), with unique slug identifiers and minimal metadata used for display and policy.

Columns (MVP):
- `slug TEXT PRIMARY KEY` (e.g., `slack`, `github`, `jira`, `google_drive`, `google_calendar`, `gmail`, `zoho_cliq`, `zoho_mail`)
- `display_name TEXT NOT NULL`
- `auth_type TEXT NOT NULL` (enum-like string; e.g., `oauth2`, `webhook-only`)
- `created_at TIMESTAMPTZ NOT NULL DEFAULT now()`
- `updated_at TIMESTAMPTZ NOT NULL DEFAULT now()`

Constraints/Indices:
- Primary key on `slug`

#### Scenario: Insert provider succeeds
- GIVEN a new `slug` not present in the table
- WHEN inserting into `providers (slug, display_name, auth_type)`
- THEN the row is created and timestamps are set

#### Scenario: Duplicate slug rejected
- GIVEN an existing `providers.slug = 'github'`
- WHEN inserting another row with `slug = 'github'`
- THEN the operation fails due to primary key violation

### Requirement: Connection Entity Schema
The `metadata` JSONB column MUST support a `sync` object with the following scheduler fields:
- `interval_seconds` (integer, optional, bounded between 60 and `config.max_overridden_interval_seconds` (default 86400))
- `next_run_at` (TIMESTAMPTZ, optional, UTC)
- `last_jitter_seconds` (integer, optional, >= 0)
- `first_activated_at` (TIMESTAMPTZ, optional, UTC)

Scheduler updates MUST persist `sync.next_run_at` and `sync.last_jitter_seconds` atomically with job scheduling, and values outside the allowed range MUST be rejected.

#### Scenario: Scheduler metadata persisted atomically
- **WHEN** a scheduler tick enqueues an incremental job
- **THEN** the corresponding `connections.metadata.sync` object is updated in the same transaction with the computed `next_run_at` and `last_jitter_seconds`

#### Scenario: Interval override exceeds configuration
- **GIVEN** `config.max_overridden_interval_seconds = 3600`
- **AND** `connections.metadata.sync.interval_seconds = 7200`
- **WHEN** the scheduler persists metadata for the connection
- **THEN** it rejects the override value, stores the default interval instead, and logs a validation warning

### Requirement: Repository Layer (Providers, Connections)
The system SHALL provide a thin repository layer encapsulating SeaORM operations for providers and connections with clear, tenant-aware methods.

Providers repository (MVP):
- `get_by_slug(slug)` → provider or not found
- `list_all()` → iterable list
- `upsert(slug, display_name, auth_type)` → insert or update for seeding

Connections repository (MVP):
- `create(conn)` → creates a connection; enforces unique tuple
- `get_by_id(id)` → returns connection by id
- `find_by_unique(tenant_id, provider_slug, external_id)` → unique lookup
- `list_by_tenant_provider(tenant_id, provider_slug, limit, cursor?)` → paginated listing
- `update_tokens_status(id, tokens?, status?, expires_at?)` → partial update

#### Scenario: Tenant-scoped listing
- GIVEN multiple connections across tenants
- WHEN listing with `tenant_id = T1` and `provider_slug='github'`
- THEN only connections for `(T1, 'github')` are returned

#### Scenario: Upsert providers for seeding
- WHEN calling `upsert` for `slack`, `github`, `jira`, `google_drive`, `google_calendar`, `gmail`, `zoho_cliq`, `zoho_mail`
- THEN rows exist for each slug with latest display name/auth type values

### Requirement: Signal Entity Schema
The system SHALL define a `signals` table storing normalized events emitted by connectors, tenant‑scoped and queryable by provider, kind, and time.

Columns (MVP):
- `id UUID PRIMARY KEY NOT NULL`
- `tenant_id UUID NOT NULL REFERENCES tenants(id) ON DELETE CASCADE`
- `provider_slug TEXT NOT NULL REFERENCES providers(slug)`
- `connection_id UUID NOT NULL REFERENCES connections(id) ON DELETE CASCADE`
- `kind TEXT NOT NULL` (normalized event kind, e.g., `issue_created`, `pr_merged`, `message_posted`)
- `occurred_at TIMESTAMPTZ NOT NULL` (provider event timestamp)
- `received_at TIMESTAMPTZ NOT NULL DEFAULT now()` (time processed by system)
- `payload JSONB NOT NULL` (normalized event payload)
- `dedupe_key TEXT NULL` (optional, for future idempotency logic)
- `created_at TIMESTAMPTZ NOT NULL DEFAULT now()`
- `updated_at TIMESTAMPTZ NOT NULL DEFAULT now()`

Indices:
- `(tenant_id, provider_slug, occurred_at DESC)` for provider time‑range queries
- `(tenant_id, kind, occurred_at DESC)` for kind‑filtered queries
- `(connection_id, occurred_at DESC)` for per‑connection exploration
- `(tenant_id, provider_slug, dedupe_key)` (non‑unique, to support future de‑dupe checks)

#### Scenario: Insert signal succeeds
- GIVEN an existing tenant, provider, and connection
- WHEN inserting a new signal row
- THEN the row is created and timestamps are set appropriately

#### Scenario: Query by tenant and kind is efficient
- GIVEN many signals across kinds and providers
- WHEN querying by `(tenant_id, kind)` ordered by `occurred_at DESC`
- THEN the database uses the composite index and returns results quickly

### Requirement: SyncJob Entity Schema
The system SHALL define a `sync_jobs` table representing scheduled or webhook‑triggered units of work for connectors, tenant‑scoped with status, cursors, and timing metadata.

Columns (MVP):
- `id UUID PRIMARY KEY NOT NULL`
- `tenant_id UUID NOT NULL REFERENCES tenants(id) ON DELETE CASCADE`
- `provider_slug TEXT NOT NULL REFERENCES providers(slug)`
- `connection_id UUID NOT NULL REFERENCES connections(id) ON DELETE CASCADE`
- `job_type TEXT NOT NULL` (e.g., `full`, `incremental`, `webhook`)
- `status TEXT NOT NULL DEFAULT 'queued'` (e.g., `queued`, `running`, `succeeded`, `failed`)
- `priority SMALLINT NOT NULL DEFAULT 0`
- `attempts INT NOT NULL DEFAULT 0`
- `scheduled_at TIMESTAMPTZ NOT NULL DEFAULT now()`
- `retry_after TIMESTAMPTZ NULL` (next eligible time after backoff)
- `started_at TIMESTAMPTZ NULL`
- `finished_at TIMESTAMPTZ NULL`
- `cursor JSONB NULL` (opaque provider cursor)
- `error JSONB NULL` (structured failure details)
- `created_at TIMESTAMPTZ NOT NULL DEFAULT now()`
- `updated_at TIMESTAMPTZ NOT NULL DEFAULT now()`

Indices:
- `(status, scheduled_at, priority DESC)` for picking the next ready job
- `(tenant_id, provider_slug, status, scheduled_at)` for tenant/provider queue views
- `(connection_id, status, scheduled_at)` for per‑connection queue operations

#### Scenario: Queue job and pick order by priority and time
- GIVEN multiple `queued` jobs with varying `scheduled_at` and `priority`
- WHEN selecting next job ordered by highest `priority` and earliest `scheduled_at`
- THEN the index supports efficient retrieval

#### Scenario: Retry scheduling
- GIVEN a `failed` job with backoff
- WHEN setting `retry_after` in the future
- THEN job pickers exclude it until `retry_after <= now()`

### Requirement: Sync Jobs Partial Unique Index
The system MUST create a partial unique index on `sync_jobs` to prevent duplicate interval jobs per connection. The index enforces uniqueness across `(connection_id, job_type)` only for rows with `status IN ('queued', 'running')`.

Index definition:
```sql
CREATE UNIQUE INDEX idx_sync_jobs_connection_type_status 
ON sync_jobs (connection_id, job_type) 
WHERE status IN ('queued', 'running');
```

This index MUST be created in a migration script and ensures that:
- Only one interval job can be queued/running per connection at a time
- Failed/completed jobs don't count toward the uniqueness constraint
- Multiple different job types (e.g., 'incremental', 'full') can coexist for the same connection

#### Scenario: Duplicate interval job prevented by index
- **GIVEN** an incremental sync_job exists for connection `C` with `status = 'queued'`
- **WHEN** the scheduler attempts to enqueue another incremental job for `C`
- **THEN** the database rejects the insert with a unique constraint violation
- **AND** the scheduler treats this as a no-op and continues without error

#### Scenario: Different job types allowed
- **GIVEN** connection `C` has a running incremental sync_job
- **WHEN** the scheduler attempts to enqueue a full sync job for `C`
- **THEN** the insert succeeds because job_type differs ('incremental' vs 'full')
- **AND** both jobs can proceed concurrently

#### Scenario: Completed jobs don't block new scheduling
- **GIVEN** connection `C` has an incremental sync_job with `status = 'completed'`
- **WHEN** the scheduler attempts to enqueue a new incremental job for `C`
- **THEN** the insert succeeds because the completed job doesn't match the index filter
- **AND** the scheduler proceeds with normal scheduling logic

