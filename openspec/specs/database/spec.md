# database Specification

## Purpose
Define the database layer for the Connectors service, including Postgres connectivity, migration execution, and the baseline tenants schema required for future capabilities.
## Requirements
### Requirement: Postgres Connection Management
The system SHALL use SeaORM to manage a connection pool to Postgres using `POBLYSH_DATABASE_URL`. Connections MUST be established during startup and made available to request handlers and background tasks.

Pool behavior (MVP):
- Default max connections: 10 (configurable)
- Acquire timeout: 5s (configurable)
- Retry on transient errors with backoff during startup initialization

#### Scenario: Connects with valid database URL
- GIVEN `POBLYSH_DATABASE_URL` points to a reachable Postgres instance
- WHEN the service starts
- THEN a `DatabaseConnection` pool is created and stored in application state

#### Scenario: Invalid credentials fail fast
- GIVEN `POBLYSH_DATABASE_URL` contains wrong credentials
- WHEN the service starts
- THEN initialization fails with an actionable error indicating authentication failed

### Requirement: Migration Runner
The system MUST include SeaORM Migrator and apply pending migrations. For profiles `local` and `test`, migrations SHALL run automatically on startup. For other profiles (e.g., `dev`, `prod`), migrations SHOULD be executed explicitly via a CLI entry or deployment step.

#### Scenario: Local profile runs migrations on startup
- GIVEN `POBLYSH_PROFILE=local`
- AND the database is empty
- WHEN the service starts
- THEN the migrator applies all pending migrations successfully

#### Scenario: Idempotent reruns
- GIVEN the database is already at the latest migration
- WHEN the migrator runs again
- THEN it completes without altering existing schema or data

### Requirement: Baseline Schema (Tenants)
The baseline schema SHALL include a `tenants` table to support tenant isolation across all future entities.

Columns (MVP):
- `id UUID PRIMARY KEY NOT NULL` (generated by application as UUID v4)
- `name TEXT NULL`
- `created_at TIMESTAMPTZ NOT NULL DEFAULT now()`

Constraints:
- Primary key on `id`

#### Scenario: Insert tenant succeeds
- GIVEN a newly generated UUID v4 `id`
- WHEN inserting into `tenants (id, name)`
- THEN the row is created and `created_at` is set automatically

#### Scenario: Duplicate id rejected
- GIVEN an existing row in `tenants` with `id = X`
- WHEN inserting another row with `id = X`
- THEN the operation fails due to primary key violation

### Requirement: Provider Entity Schema
The system SHALL define a `providers` table representing supported integration providers (global catalog), with unique slug identifiers and minimal metadata used for display and policy.

Columns (MVP):
- `slug TEXT PRIMARY KEY` (e.g., `slack`, `github`, `jira`, `google_drive`, `google_calendar`, `gmail`, `zoho_cliq`, `zoho_mail`)
- `display_name TEXT NOT NULL`
- `auth_type TEXT NOT NULL` (enum-like string; e.g., `oauth2`, `webhook-only`)
- `created_at TIMESTAMPTZ NOT NULL DEFAULT now()`
- `updated_at TIMESTAMPTZ NOT NULL DEFAULT now()`

Constraints/Indices:
- Primary key on `slug`

#### Scenario: Insert provider succeeds
- GIVEN a new `slug` not present in the table
- WHEN inserting into `providers (slug, display_name, auth_type)`
- THEN the row is created and timestamps are set

#### Scenario: Duplicate slug rejected
- GIVEN an existing `providers.slug = 'github'`
- WHEN inserting another row with `slug = 'github'`
- THEN the operation fails due to primary key violation

### Requirement: Connection Entity Schema
The system SHALL define a `connections` table representing tenant-scoped authorizations for a given provider and external account/workspace. Connections MUST be uniquely identified per `(tenant_id, provider_slug, external_id)`.

Columns (MVP):
- `id UUID PRIMARY KEY NOT NULL`
- `tenant_id UUID NOT NULL REFERENCES tenants(id) ON DELETE CASCADE`
- `provider_slug TEXT NOT NULL REFERENCES providers(slug)`
- `external_id TEXT NOT NULL` (provider-specific account/workspace/user/install id)
- `status TEXT NOT NULL DEFAULT 'active'` (enum-like: `active`, `revoked`, `error`)
- `scopes TEXT[] NULL` (granted scopes if applicable)
- `access_token_ciphertext BYTEA NULL` (opaque; to be encrypted by crypto module in later change)
- `refresh_token_ciphertext BYTEA NULL`
- `expires_at TIMESTAMPTZ NULL`
- `metadata JSONB NULL` (opaque provider-specific details)
- `created_at TIMESTAMPTZ NOT NULL DEFAULT now()`
- `updated_at TIMESTAMPTZ NOT NULL DEFAULT now()`

Constraints/Indices:
- Unique index on `(tenant_id, provider_slug, external_id)`
- Index on `tenant_id`

#### Scenario: Insert connection succeeds
- GIVEN an existing tenant and provider
- WHEN inserting a new `(tenant_id, provider_slug, external_id)`
- THEN the row is created and timestamps are set

#### Scenario: Duplicate tuple rejected
- GIVEN a connection exists for `(tenant_id=T, provider_slug='github', external_id='org:42')`
- WHEN inserting another row with the same tuple
- THEN the operation fails due to unique constraint

#### Scenario: Tenant isolation enforced
- GIVEN two tenants `T1`, `T2`
- WHEN each inserts a connection with the same `(provider_slug, external_id)`
- THEN both rows exist because uniqueness is scoped per tenant

### Requirement: Repository Layer (Providers, Connections)
The system SHALL provide a thin repository layer encapsulating SeaORM operations for providers and connections with clear, tenant-aware methods.

Providers repository (MVP):
- `get_by_slug(slug)` → provider or not found
- `list_all()` → iterable list
- `upsert(slug, display_name, auth_type)` → insert or update for seeding

Connections repository (MVP):
- `create(conn)` → creates a connection; enforces unique tuple
- `get_by_id(id)` → returns connection by id
- `find_by_unique(tenant_id, provider_slug, external_id)` → unique lookup
- `list_by_tenant_provider(tenant_id, provider_slug, limit, cursor?)` → paginated listing
- `update_tokens_status(id, tokens?, status?, expires_at?)` → partial update

#### Scenario: Tenant-scoped listing
- GIVEN multiple connections across tenants
- WHEN listing with `tenant_id = T1` and `provider_slug='github'`
- THEN only connections for `(T1, 'github')` are returned

#### Scenario: Upsert providers for seeding
- WHEN calling `upsert` for `slack`, `github`, `jira`, `google_drive`, `google_calendar`, `gmail`, `zoho_cliq`, `zoho_mail`
- THEN rows exist for each slug with latest display name/auth type values

### Requirement: Signal Entity Schema
The system SHALL define a `signals` table storing normalized events emitted by connectors, tenant‑scoped and queryable by provider, kind, and time.

Columns (MVP):
- `id UUID PRIMARY KEY NOT NULL`
- `tenant_id UUID NOT NULL REFERENCES tenants(id) ON DELETE CASCADE`
- `provider_slug TEXT NOT NULL REFERENCES providers(slug)`
- `connection_id UUID NOT NULL REFERENCES connections(id) ON DELETE CASCADE`
- `kind TEXT NOT NULL` (normalized event kind, e.g., `issue_created`, `pr_merged`, `message_posted`)
- `occurred_at TIMESTAMPTZ NOT NULL` (provider event timestamp)
- `received_at TIMESTAMPTZ NOT NULL DEFAULT now()` (time processed by system)
- `payload JSONB NOT NULL` (normalized event payload)
- `dedupe_key TEXT NULL` (optional, for future idempotency logic)
- `created_at TIMESTAMPTZ NOT NULL DEFAULT now()`
- `updated_at TIMESTAMPTZ NOT NULL DEFAULT now()`

Indices:
- `(tenant_id, provider_slug, occurred_at DESC)` for provider time‑range queries
- `(tenant_id, kind, occurred_at DESC)` for kind‑filtered queries
- `(connection_id, occurred_at DESC)` for per‑connection exploration
- `(tenant_id, provider_slug, dedupe_key)` (non‑unique, to support future de‑dupe checks)

#### Scenario: Insert signal succeeds
- GIVEN an existing tenant, provider, and connection
- WHEN inserting a new signal row
- THEN the row is created and timestamps are set appropriately

#### Scenario: Query by tenant and kind is efficient
- GIVEN many signals across kinds and providers
- WHEN querying by `(tenant_id, kind)` ordered by `occurred_at DESC`
- THEN the database uses the composite index and returns results quickly

### Requirement: SyncJob Entity Schema
The system SHALL define a `sync_jobs` table representing scheduled or webhook‑triggered units of work for connectors, tenant‑scoped with status, cursors, and timing metadata.

Columns (MVP):
- `id UUID PRIMARY KEY NOT NULL`
- `tenant_id UUID NOT NULL REFERENCES tenants(id) ON DELETE CASCADE`
- `provider_slug TEXT NOT NULL REFERENCES providers(slug)`
- `connection_id UUID NOT NULL REFERENCES connections(id) ON DELETE CASCADE`
- `job_type TEXT NOT NULL` (e.g., `full`, `incremental`, `webhook`)
- `status TEXT NOT NULL DEFAULT 'queued'` (e.g., `queued`, `running`, `succeeded`, `failed`)
- `priority SMALLINT NOT NULL DEFAULT 0`
- `attempts INT NOT NULL DEFAULT 0`
- `scheduled_at TIMESTAMPTZ NOT NULL DEFAULT now()`
- `retry_after TIMESTAMPTZ NULL` (next eligible time after backoff)
- `started_at TIMESTAMPTZ NULL`
- `finished_at TIMESTAMPTZ NULL`
- `cursor JSONB NULL` (opaque provider cursor)
- `error JSONB NULL` (structured failure details)
- `created_at TIMESTAMPTZ NOT NULL DEFAULT now()`
- `updated_at TIMESTAMPTZ NOT NULL DEFAULT now()`

Indices:
- `(status, scheduled_at, priority DESC)` for picking the next ready job
- `(tenant_id, provider_slug, status, scheduled_at)` for tenant/provider queue views
- `(connection_id, status, scheduled_at)` for per‑connection queue operations

#### Scenario: Queue job and pick order by priority and time
- GIVEN multiple `queued` jobs with varying `scheduled_at` and `priority`
- WHEN selecting next job ordered by highest `priority` and earliest `scheduled_at`
- THEN the index supports efficient retrieval

#### Scenario: Retry scheduling
- GIVEN a `failed` job with backoff
- WHEN setting `retry_after` in the future
- THEN job pickers exclude it until `retry_after <= now()`

